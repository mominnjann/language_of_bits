Terms & Definitions â€” Language of Bits

This file lists short, practical definitions of common terms used when working with binary information.

Bit
- The smallest unit of information; a value of 0 or 1.

Byte
- A group of 8 bits (commonly used as the basic addressable unit in memory).

Boolean
- A variable or value that can be either true/false or 1/0.

Boolean algebra
- The algebra of logical values with operators such as AND, OR, and NOT.

Binary number
- A number represented in base 2 using only 0 and 1.

Logic gates
- Hardware or logical primitives (AND, OR, NOT, XOR, NAND, NOR) that compute boolean functions.

Bitwise operations
- Operations applying logical or arithmetic operations on corresponding bits (AND, OR, XOR, NOT, shifts).

Endianness
- The byte order used to represent multi-byte values: little-endian (least-significant byte first) or big-endian (most-significant byte first).

Encoding
- A mapping between symbols (text, images, sound) and binary representations (examples: ASCII, UTF-8, RGB, PCM).

Bitmask
- A binary pattern used to extract, set, or clear specific bits in a value.

Parity
- A simple error-detection bit indicating whether the number of 1 bits is even or odd.

Hamming code
- A family of error-detecting and error-correcting codes that add redundant bits to enable single-bit error correction.

Entropy (information)
- A measure of unpredictability or information content in a source (lower entropy: more predictable).

Machine code / instruction
- Low-level binary-encoded commands that a CPU executes.